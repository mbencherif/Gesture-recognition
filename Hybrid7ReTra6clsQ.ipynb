{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hybrid7ReTra6clsQ.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mingweihe/HandGestureRecognition/blob/master/Hybrid7ReTra6clsQ.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "YJaLeZZIbe-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#------------------------------------------------\n",
        "#    Training and Test data set preparation\n",
        "#------------------------------------------------\n",
        "!rm -rf checkpoints && rm -rf *.zip\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# 2. Downloading hafl trained model\n",
        "local_download_path = os.path.expanduser('~')\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'19lVhhib3mhWMW_eQsFFoOQLVsmfN6SB3' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)\n",
        "\n",
        "!unzip /content/*.zip\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8h00KEOGc-P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lReNlvi6VZQV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set parameters\n",
        "chkp_dir = os.path.join('checkpoints', 'cnnGesture')\n",
        "chkp_name = 'model'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WtYcPcn7VZQY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# images plotting function\n",
        "def plotImg(imgs, titles = None):\n",
        "    fig = plt.figure()\n",
        "    for i in range(len(imgs)):\n",
        "        ax = fig.add_subplot(1, len(imgs), i + 1)\n",
        "        if titles != None:\n",
        "            ax.title.set_text(titles[i])\n",
        "        plt.imshow(imgs[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSEdoLfqGdAV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# restore checkpoint\n",
        "sessRes = tf.Session()\n",
        "saver = tf.train.import_meta_graph(os.path.join(chkp_dir, 'model.meta'))\n",
        "saver.restore(sessRes, tf.train.latest_checkpoint(chkp_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SqvCis4YHpSX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# variable and tensor initialization\n",
        "img_holder = sessRes.graph.get_tensor_by_name('img_holder:0')\n",
        "lbl_holder =sessRes.graph.get_tensor_by_name('lbl_holder:0')\n",
        "loss=sessRes.graph.get_tensor_by_name('loss:0')\n",
        "accuracy=tf.get_default_graph().get_tensor_by_name('accuracy:0')\n",
        "optimizer=tf.get_default_graph().get_tensor_by_name('optimizer:0')\n",
        "predict=tf.get_default_graph().get_tensor_by_name('prediction:0')\n",
        "train = sessRes.graph.get_tensor_by_name('train_bool:0')\n",
        "log_dir = sessRes.run('log_dir:0').decode('utf-8')\n",
        "test_total = sessRes.run('test_total:0')\n",
        "train_total = sessRes.run('train_total:0')\n",
        "num_epochs = sessRes.run('num_epochs:0')\n",
        "batch_size = sessRes.run('batch_size:0')\n",
        "global_step = sessRes.run('global_step:0')\n",
        "global_epoch = int(global_step*batch_size/train_total)\n",
        "# # tensorboard\n",
        "# summary_loss = tf.summary.scalar('loss', loss)\n",
        "# summary_accuracy = tf.summary.scalar('accuracy', accuracy)\n",
        "# merged_op = tf.summary.merge([summary_loss, summary_accuracy])\n",
        "# # Create FileWriter\n",
        "# file_writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())\n",
        "print('number of epoch for each training:', num_epochs)\n",
        "print('batch size of each step:', batch_size)\n",
        "print('total test set:', test_total)\n",
        "print('total train set:', train_total)\n",
        "print('global steps:', global_step)\n",
        "print('global epochs:', global_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u1LGYolfIZSd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# session to exexute queuing\n",
        "# restore checkpoint\n",
        "sess4Q = tf.Session()\n",
        "saver4Q = tf.train.import_meta_graph(os.path.join(chkp_dir, 'model.meta'))\n",
        "saver4Q.restore(sess4Q, tf.train.latest_checkpoint(chkp_dir))\n",
        "batchTrainData = sess4Q.graph.get_tensor_by_name('batchTrainData:0')\n",
        "batchTrainLabel = sess4Q.graph.get_tensor_by_name('batchTrainData:1')\n",
        "totalTestData = sess4Q.graph.get_tensor_by_name('totalTestData:0')\n",
        "totalTestLabel = sess4Q.graph.get_tensor_by_name('totalTestData:1')\n",
        "sess4Q.run(tf.local_variables_initializer())\n",
        "coord=tf.train.Coordinator()\n",
        "threads=tf.train.start_queue_runners(sess=sess4Q,coord=coord)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mj40oy8Oev6O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    test_images,test_labels = sess4Q.run([totalTestData, totalTestLabel])\n",
        "    batch_i = 1\n",
        "    total_batch = 0\n",
        "    local_epoch = 1\n",
        "    start_time = time.time()\n",
        "    while not coord.should_stop():\n",
        "        imgs, labels = sess4Q.run([batchTrainData,batchTrainLabel])\n",
        "        sessRes.run(optimizer, feed_dict={img_holder:imgs,lbl_holder:labels,train: True})\n",
        "#         loss_val, accuracy_val, summary = sessRes.run([loss, accuracy, merged_op],feed_dict={img_holder:imgs,lbl_holder:labels,train: False})\n",
        "#         # Print the summary data\n",
        "#         file_writer.add_summary(summary)\n",
        "        if batch_i % 100 == 0:\n",
        "            loss_val, accuracy_val = sessRes.run([loss, accuracy],feed_dict={img_holder:imgs,lbl_holder:labels,train: False})\n",
        "            test_accuracy = sessRes.run(accuracy, feed_dict={img_holder:test_images[269:369],lbl_holder:test_labels[269:369],train: False})\n",
        "#             file_writer.flush()\n",
        "            # save training result\n",
        "            saver.save(sessRes, os.path.join(chkp_dir, chkp_name))\n",
        "            duration = time.time() - start_time\n",
        "            print('epoch:', str(local_epoch)+ '/' +str(num_epochs), 'batch:', batch_i, 'loss:', \n",
        "                  loss_val, 'cur_accuracy:', accuracy_val, 'test_accuracy:', test_accuracy, 'duration: %.3fs' % duration)\n",
        "            start_time = time.time()\n",
        "        batch_i += 1\n",
        "        total_batch += batch_size\n",
        "        if total_batch >= train_total:\n",
        "            local_epoch += 1\n",
        "            total_batch = 0\n",
        "            batch_i = 1\n",
        "except tf.errors.OutOfRangeError:\n",
        "    print('Done.')\n",
        "except Exception as e:\n",
        "    coord.request_stop(e)\n",
        "finally:\n",
        "    coord.request_stop()\n",
        "coord.join(threads)\n",
        "sess4Q.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xG409xEyGdAW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "begin = random.randrange(0, test_total-10)\n",
        "end = begin + 10\n",
        "res, accuracy_val = sessRes.run([predict, accuracy], feed_dict = {img_holder: test_images[begin:end], lbl_holder:test_labels[begin:end], train: False})\n",
        "titles = [str(np.argmax(test_labels[i])) + '->' + str(res[i-begin]) for i in range(begin, end)] \n",
        "plotImg(test_images[begin:end], titles)\n",
        "print('Accuracy:', accuracy_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QKzVBejXTsYG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sessRes.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "STIvxtMuGdAc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # memory footprint support libraries/code\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip install gputil\n",
        "# !pip install psutil\n",
        "# !pip install humanize\n",
        "# import psutil\n",
        "# import humanize\n",
        "# import os\n",
        "# import GPUtil as GPU\n",
        "# GPUs = GPU.getGPUs()\n",
        "# # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "# gpu = GPUs[0]\n",
        "# def printm():\n",
        "#  process = psutil.Process(os.getpid())\n",
        "#  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "#  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "# printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNY_y6KMNlrf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#------------------------------------------------\n",
        "#    Replace google drive trained model with new one\n",
        "#------------------------------------------------\n",
        "!rm -rf Hybrid7.zip\n",
        "!zip -r Hybrid7.zip checkpoints\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'19lVhhib3mhWMW_eQsFFoOQLVsmfN6SB3' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # delete old one\n",
        "  fname = f['title']\n",
        "  print('Deleting', fname)\n",
        "  f.Delete()\n",
        "file_metadata = {'title': 'Hybrid7.zip', \"parents\": [{\"id\": '19lVhhib3mhWMW_eQsFFoOQLVsmfN6SB3', \"kind\": \"drive#childList\"}]}\n",
        "# Create & upload a file.\n",
        "uploaded = drive.CreateFile(file_metadata)\n",
        "uploaded.SetContentFile('Hybrid7.zip')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LAGbyy4ltk56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}